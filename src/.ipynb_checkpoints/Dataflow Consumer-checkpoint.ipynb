{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4d1f7a-6479-431f-84f6-cd0cd90589aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Authentication and Installing Beam\n",
    "!gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb14eaa-e32c-43c4-bca5-921100bf69c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!!pip install --upgrade -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1e0ab1-31f5-4efe-8d46-8cf9e7aad096",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install virtualenv\n",
    "!python -m venv beam-env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d7b08c-980e-4961-b124-81e73dc699d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activate Virtual Environment\n",
    "!beam-env\\Scripts\\activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7bf9d2-e809-445f-942a-db836ae2718c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking Beam Version\n",
    "import apache_beam as beam\n",
    "beam.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88875c2b-d240-46cf-8032-a82960638f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking Default Credentials\n",
    "from google.auth import default\n",
    "\n",
    "credentials, project = default()\n",
    "print(f\"Active project: {project}\")\n",
    "print(f\"Credentials: {credentials}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4da05bf-8781-49d7-a4b1-e498629e7638",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions, WorkerOptions\n",
    "import json\n",
    "from apache_beam.transforms.sql import SqlTransform\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# Configuration constants\n",
    "PROJECT_ID = \"coastal-throne-433510-a5\"\n",
    "DATASET_ID = \"pubsub_data\"\n",
    "TABLE_NAME = \"employee_salaries_with_bonus_tbl\"\n",
    "BUCKET_NAME = \"coastal-throne-433510-a5-dataflow-bucket\"\n",
    "REGION = \"us-central1\"\n",
    "SUBSCRIPTION = f\"projects/{PROJECT_ID}/subscriptions/employee_sal_data_sub\"\n",
    "TEMP_LOCATION = f\"gs://{BUCKET_NAME}/temp\"\n",
    "STAGING_LOCATION = f\"gs://{BUCKET_NAME}/staging\"\n",
    "\n",
    "# Add schema using beam.Row\n",
    "def add_schema(record):\n",
    "    return beam.Row(\n",
    "        first_name=str(record['First Name']),\n",
    "        gender=str(record['Gender']),\n",
    "        start_date=str(record['Start Date']),\n",
    "        last_login_time=str(record['Last Login Time']),\n",
    "        salary=float(record['Salary']),\n",
    "        bonus_percent=float(record['Bonus %']),\n",
    "        senior_management=bool(record['Senior Management']),\n",
    "        team=str(record['Team'])\n",
    "    )\n",
    "\n",
    "# Convert beam.Row to a dictionary for BigQuery serialization, adding insert_timestamp\n",
    "def row_to_dict(row):\n",
    "    return {\n",
    "        'insert_timestamp': datetime.utcnow().isoformat(),\n",
    "        'first_name': row.first_name,\n",
    "        'gender': row.gender,\n",
    "        'start_date': row.start_date,\n",
    "        'last_login_time': row.last_login_time,\n",
    "        'salary': row.salary,\n",
    "        'bonus_percent': row.bonus_percent,\n",
    "        'bonus_actual': row.bonus_actual,\n",
    "        'senior_management': row.senior_management,\n",
    "        'team': row.team\n",
    "    }\n",
    "\n",
    "def run():\n",
    "    # Define your pipeline options with autoscaling limited to 2 workers\n",
    "    options = PipelineOptions(\n",
    "        runner='DataflowRunner',\n",
    "        project=PROJECT_ID,\n",
    "        temp_location=TEMP_LOCATION,\n",
    "        staging_location=STAGING_LOCATION,\n",
    "        region=REGION,\n",
    "        streaming=True,\n",
    "        save_main_session=True\n",
    "    )\n",
    "    worker_options = options.view_as(WorkerOptions)\n",
    "    worker_options.max_num_workers = 2\n",
    "\n",
    "    # Create the pipeline\n",
    "    with beam.Pipeline(options=options) as p:\n",
    "        # Read data from Pub/Sub\n",
    "        pubsub_data = (\n",
    "            p\n",
    "            | \"Read from PubSub\" >> beam.io.ReadFromPubSub(subscription=SUBSCRIPTION)\n",
    "            | \"Parse JSON\" >> beam.Map(lambda x: json.loads(x))\n",
    "            | \"Add Schema\" >> beam.Map(add_schema)\n",
    "        )\n",
    "\n",
    "        # Apply a fixed window of 20 seconds\n",
    "        windowed_data = (\n",
    "            pubsub_data\n",
    "            | \"Apply 20-second Window\" >> beam.WindowInto(beam.window.FixedWindows(20))\n",
    "        )\n",
    "\n",
    "        # Use an SQL transform to calculate the bonus\n",
    "        bonus_calculation_query = \"\"\"\n",
    "        SELECT \n",
    "            first_name, \n",
    "            gender, \n",
    "            start_date, \n",
    "            last_login_time, \n",
    "            salary, \n",
    "            bonus_percent, \n",
    "            (bonus_percent / 100) * salary AS bonus_actual, \n",
    "            senior_management, \n",
    "            team\n",
    "        FROM PCOLLECTION\n",
    "        \"\"\"\n",
    "        \n",
    "        standardized_data = (\n",
    "            windowed_data\n",
    "            | \"Calculate Bonus\" >> SqlTransform(bonus_calculation_query)\n",
    "        )\n",
    "\n",
    "        # Convert rows to dictionaries before writing to BigQuery\n",
    "        standardized_data_dict = standardized_data | \"Row to Dict\" >> beam.Map(row_to_dict)\n",
    "\n",
    "        # Write to BigQuery - Employee Salaries with insert_timestamp\n",
    "        standardized_data_dict | \"Write to BigQuery - Employee Salaries\" >> beam.io.WriteToBigQuery(\n",
    "            table=f\"{PROJECT_ID}:{DATASET_ID}.{TABLE_NAME}\",\n",
    "            schema=(\n",
    "                'insert_timestamp:TIMESTAMP, first_name:STRING, gender:STRING, start_date:STRING, '\n",
    "                'last_login_time:STRING, salary:FLOAT, bonus_percent:FLOAT, bonus_actual:FLOAT, '\n",
    "                'senior_management:BOOLEAN, team:STRING'\n",
    "            ),\n",
    "            write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,\n",
    "            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED\n",
    "        )\n",
    "\n",
    "# Execute the pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605c0737-9de6-4325-bf24-aa2e1413b1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!deactivate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
